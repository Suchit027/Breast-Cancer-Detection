{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ce3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import defaultdict\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a5701",
   "metadata": {},
   "source": [
    "### Finding different shapes and channels of images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05d8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape length - 639 and mask shape length - 639\n",
      "number of channels - 3\n",
      "channels - {'RGBA', 'RGB', '1'}\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'D:\\Suchit\\Breast-Cancer-Detection\\Dataset_BUSI_with_GT'\n",
    "image_shapes, mask_shapes = set(), set()\n",
    "channels = set()\n",
    "images, masks = [], []\n",
    "for folder in os.listdir(folder_path):\n",
    "    for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "        with Image.open(os.path.join(folder_path, folder, image)) as img:\n",
    "            if 'mask' in image:\n",
    "                mask_shapes.add(img.size)\n",
    "                masks.append(os.path.join(folder_path, folder, image))\n",
    "            else:\n",
    "                image_shapes.add(img.size)\n",
    "                images.append(os.path.join(folder_path, folder, image))\n",
    "            channels.add(img.mode)\n",
    "print(\n",
    "    f'image shape length - {len(image_shapes)} and mask shape length - {len(mask_shapes)}')\n",
    "print(f'number of channels - {len(channels)}')\n",
    "print(f'channels - {channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36eff8b",
   "metadata": {},
   "source": [
    "### Finding the average height and width of the images to resize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64e5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average height = 501.4525641025641\n",
      "average width = 615.6794871794872\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'D:\\Suchit\\Breast-Cancer-Detection\\Dataset_BUSI_with_GT'\n",
    "height, width, num_samples = 0.0, 0.0, 0.0\n",
    "for folder in os.listdir(folder_path):\n",
    "    for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "        with Image.open(os.path.join(folder_path, folder, image)) as img:\n",
    "            if 'mask' not in image:\n",
    "                size = img.size\n",
    "                height += size[1]\n",
    "                width += size[0]\n",
    "                num_samples += 1\n",
    "height /= num_samples\n",
    "width /= num_samples\n",
    "print(f'average height = {height}')\n",
    "print(f'average width = {width}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2f6cd",
   "metadata": {},
   "source": [
    "Height and width is too large. Taking 256 * 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f3200",
   "metadata": {},
   "source": [
    "### Finding the mean and standard deviation of the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf7916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_from_pil(x: np.ndarray) -> np.ndarray:\n",
    "    if x.ndim == 2:\n",
    "        return cv2.cvtColor(x, cv2.COLOR_GRAY2RGB)\n",
    "    if x.ndim == 3:\n",
    "        c = x.shape[2]\n",
    "        if c == 1:\n",
    "            return cv2.cvtColor(x, cv2.COLOR_GRAY2RGB)\n",
    "        elif c == 3:\n",
    "            return x  # already RGB\n",
    "        elif c == 4:\n",
    "            # RGBA -> RGB (drops alpha)\n",
    "            return cv2.cvtColor(x, cv2.COLOR_RGBA2RGB)\n",
    "    return x[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8071d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating RGB statistics...\n",
      "Found 780 images for statistics calculation\n",
      "RGB mean  = [0.32795798778533936, 0.3279508650302887, 0.32790568470954895]\n",
      "RGB std   = [0.21965765953063965, 0.21965673565864563, 0.2196434587240219]\n",
      "Per-channel formatted -> mean: R 0.3280, G 0.3280, B 0.3279 | std:  R 0.2197, G 0.2197, B 0.2196\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "def calculate_rgb_stats(folder_path, resize_shape=(256, 256), batch_size=64, num_workers=0):\n",
    "    print(\"Calculating RGB statistics...\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_shape, interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),  # -> [0,1], shape (C,H,W) with C=3 since we .convert('RGB')\n",
    "    ])\n",
    "\n",
    "    class ImageData(Dataset):\n",
    "        def __init__(self, images, transform):\n",
    "            self.images = images\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img_path = self.images[index]\n",
    "            # Force RGB: handles grayscale (L) and RGBA (drops alpha) gracefully\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            return self.transform(image)\n",
    "\n",
    "    # Collect image paths (exclude anything with 'mask' in the filename)\n",
    "    images = []\n",
    "    for folder in os.listdir(folder_path):\n",
    "        full = os.path.join(folder_path, folder)\n",
    "        if not os.path.isdir(full):\n",
    "            continue\n",
    "        for fname in os.listdir(full):\n",
    "            if 'mask' in fname.lower():\n",
    "                continue\n",
    "            images.append(os.path.join(full, fname))\n",
    "\n",
    "    if len(images) == 0:\n",
    "        raise RuntimeError(\"No images found (after excluding masks). Check your folder structure.\")\n",
    "\n",
    "    print(f\"Found {len(images)} images for statistics calculation\")\n",
    "\n",
    "    dataset = ImageData(images, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Numerically stable: use sums and squared sums\n",
    "    sum_ = torch.zeros(3)\n",
    "    sum_sq = torch.zeros(3)\n",
    "    n_pixels = 0\n",
    "\n",
    "    for data in loader:\n",
    "        # data: (B, 3, H, W)\n",
    "        b, c, h, w = data.shape\n",
    "        n = b * h * w\n",
    "        sum_ += data.sum(dim=[0, 2, 3])\n",
    "        sum_sq += (data ** 2).sum(dim=[0, 2, 3])\n",
    "        n_pixels += n\n",
    "\n",
    "    mean = sum_ / n_pixels\n",
    "    var = (sum_sq / n_pixels) - mean ** 2\n",
    "    std = var.clamp(min=0).sqrt()\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Example usage\n",
    "rgb_mean, rgb_std = calculate_rgb_stats(folder_path, resize_shape=(256, 256))\n",
    "print(f\"RGB mean  = {rgb_mean.tolist()}\")\n",
    "print(f\"RGB std   = {rgb_std.tolist()}\")\n",
    "print(f\"Per-channel formatted -> \"\n",
    "      f\"mean: R {rgb_mean[0]:.4f}, G {rgb_mean[1]:.4f}, B {rgb_mean[2]:.4f} | \"\n",
    "      f\"std:  R {rgb_std[0]:.4f}, G {rgb_std[1]:.4f}, B {rgb_std[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696dbce",
   "metadata": {},
   "source": [
    "### Managing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97e944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, folder_path, transforms):\n",
    "        super().__init__()\n",
    "        self.images, self.masks, self.category = [], defaultdict(list), []\n",
    "        classes = {'benign': 0, 'malignant': 1, 'normal': 2}\n",
    "        self.transforms = transforms\n",
    "\n",
    "        for folder in os.listdir(folder_path):\n",
    "            for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "                img_path = os.path.join(folder_path, folder, image)\n",
    "                if 'mask' in image:\n",
    "                    self.masks[image[: image.index('_mask')]].append(img_path)\n",
    "                else:\n",
    "                    self.images.append(img_path)\n",
    "                    self.category.append(classes[folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        key = os.path.basename(image_path)[\n",
    "            : os.path.basename(image_path).index('.png')]\n",
    "        mask_paths = self.masks[key]\n",
    "\n",
    "        # Load image and masks as numpy arrays\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        masks = [np.array(Image.open(x).convert('RGB')) for x in mask_paths]\n",
    "\n",
    "        # Combine masks\n",
    "        mask = np.sum(np.stack(masks), axis=0)\n",
    "        mask = np.clip(mask, 0, 1)  # ensure binary\n",
    "\n",
    "        # Apply albumentations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask, self.category[index]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),  # -> [0,1], shape (C,H,W) with C=3 since we .convert('RGB')\n",
    "    ])\n",
    "\n",
    "dataset = Data(folder_path, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ed3d7",
   "metadata": {},
   "source": [
    "### Calculating positive weight to upweight the positive pixels in BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41b07e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() got an unexpected keyword argument 'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     16\u001b[39m count_transform = transforms.Compose([\n\u001b[32m     17\u001b[39m     transforms.Resize((\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m), interpolation= cv2.INTER_NEAREST),\n\u001b[32m     18\u001b[39m     transforms.Normalize(mean = rgb_mean.tolist(), std= rgb_std.tolist()),\n\u001b[32m     19\u001b[39m     transforms.ToTensor()\n\u001b[32m     20\u001b[39m ])\n\u001b[32m     21\u001b[39m count_dataset = Data(folder_path= folder_path, transforms= count_transform)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m pos_weight = \u001b[43mestimate_pos_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m pos_weight = pos_weight.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEstimated positive weight = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mestimate_pos_weight\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m      3\u001b[39m pos, neg = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Suchit\\Breast-Cancer-Detection\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Suchit\\Breast-Cancer-Detection\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Suchit\\Breast-Cancer-Detection\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mData.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Apply albumentations\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     augmented = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     image = augmented[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m     mask = augmented[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: Compose.__call__() got an unexpected keyword argument 'image'"
     ]
    }
   ],
   "source": [
    "def estimate_pos_weight(dataset):\n",
    "    loader = DataLoader(dataset= dataset, batch_size=16, shuffle= False)\n",
    "    pos, neg = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for _, mask, _ in loader:\n",
    "            mask = mask.flatten()\n",
    "            mask = (mask > 0)\n",
    "            pos += mask.sum().item()\n",
    "            neg += mask.numel() - mask.sum().item()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0)\n",
    "    return torch.tensor(neg / pos)\n",
    "\n",
    "# making counting transforms for counting the pos weights\n",
    "\n",
    "count_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation= cv2.INTER_NEAREST),\n",
    "    transforms.Normalize(mean = rgb_mean.tolist(), std= rgb_std.tolist()),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "count_dataset = Data(folder_path= folder_path, transforms= count_transform)\n",
    "pos_weight = estimate_pos_weight(count_dataset)\n",
    "pos_weight = pos_weight.to('cuda')\n",
    "print(f'Estimated positive weight = {pos_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fae8b0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels,\n",
    "                      kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetWithClassifier(nn.Module):\n",
    "    def __init__(self, in_channels=3, seg_out_channels=3, num_classes=3, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Decoder (with concatenations)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        self.dec4 = DoubleConv(1024, 512)   # 512 up + 512 skip\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.dec3 = DoubleConv(512, 256)    # 256 up + 256 skip\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.dec2 = DoubleConv(256, 128)    # 128 up + 128 skip\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.dec1 = DoubleConv(128, 64)     # 64 up + 64 skip\n",
    "\n",
    "        self.seg_head = nn.Conv2d(64, seg_out_channels, kernel_size=1)\n",
    "\n",
    "        # Classification head on bottleneck features\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        p1 = self.pool1(d1)\n",
    "\n",
    "        d2 = self.down2(p1)\n",
    "        p2 = self.pool2(d2)\n",
    "\n",
    "        d3 = self.down3(p2)\n",
    "        p3 = self.pool3(d3)\n",
    "\n",
    "        d4 = self.down4(p3)\n",
    "        p4 = self.pool4(d4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.up4(bn)\n",
    "        u4 = torch.cat([u4, d4], dim=1)\n",
    "        u4 = self.dec4(u4)\n",
    "\n",
    "        u3 = self.up3(u4)\n",
    "        u3 = torch.cat([u3, d3], dim=1)\n",
    "        u3 = self.dec3(u3)\n",
    "\n",
    "        u2 = self.up2(u3)\n",
    "        u2 = torch.cat([u2, d2], dim=1)\n",
    "        u2 = self.dec2(u2)\n",
    "\n",
    "        u1 = self.up1(u2)\n",
    "        u1 = torch.cat([u1, d1], dim=1)\n",
    "        u1 = self.dec1(u1)\n",
    "\n",
    "        seg_logits = self.seg_head(u1)\n",
    "        cls_logits = self.cls_head(self.gap(bn))\n",
    "        return seg_logits, cls_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd28d2",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth = 1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, output_mask, targets):\n",
    "        targets = targets.float()\n",
    "        probs = torch.sigmoid(output_mask)\n",
    "        # no. of batches\n",
    "        B = output_mask.shape[0]\n",
    "        probs_flatten = probs.view(B, -1)\n",
    "        targs_flatten = targets.view(B, -1)\n",
    "        intersection = (probs_flatten * targs_flatten).sum(dim = 1)\n",
    "        p_sum = probs_flatten.sum(dim = 1)\n",
    "        t_sum = targs_flatten.sum(dim = 1)\n",
    "        # identifying empty target images\n",
    "        empty = (t_sum == 0)\n",
    "        # standard dice for non empty\n",
    "        dice_non_empty = (2 * intersection + self.smooth) / (p_sum + t_sum + self.smooth)\n",
    "        # defining dice for empty target images\n",
    "        # if predictions also empty -> 1, else -> (smooth) / (psum + smooth)\n",
    "        dice_empty = torch.where((p_sum == 0), torch.ones_like(p_sum), (self.smooth) / (p_sum + self.smooth))\n",
    "        dice = torch.where(empty, dice_empty, dice_non_empty)\n",
    "        return 1 - dice.mean()\n",
    "    \n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, bce_pos_weight):\n",
    "        super().__init__()\n",
    "        self.seg_weight = 1.0\n",
    "        self.cls_weight = 0.5\n",
    "        self.bce_weight = 0.5\n",
    "        self.dice_weight = 0.5\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight= bce_pos_weight)\n",
    "        self.dice = DiceLoss()\n",
    "        self.crossentropy = nn.CrossEntropyLoss()\n",
    "    def forward(self, output_mask, target_mask, output_class_logits, class_logits):\n",
    "        loss_bce = self.bce(output_mask, target_mask)\n",
    "        loss_dice = self.dice(output_mask, target_mask)\n",
    "        \n",
    "        loss_mask = self.bce_weight * loss_bce + self.dice_weight * loss_dice\n",
    "\n",
    "        loss_classification = self.crossentropy(output_class_logits  , class_logits.long())\n",
    "\n",
    "        total = self.seg_weight * loss_mask + self.cls_weight * loss_classification\n",
    "\n",
    "        return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaddd36",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = UNetWithClassifier()\n",
    "model = model.to('cuda')\n",
    "\n",
    "# loss function\n",
    "criterion = MultiTaskLoss(pos_weight)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 0.001, weight_decay= 0.0001)\n",
    "\n",
    "# train test split\n",
    "n_total = len(dataset)\n",
    "n_val = int(0.2 * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val], generator= torch.Generator().manual_seed(42))\n",
    "\n",
    "# weighted random sampler\n",
    "labels = np.array([dataset.category[i] for i in train_dataset.indices])\n",
    "class_counts = np.bincount(labels, minlength= 3)\n",
    "class_weights = class_counts.sum() / (len(class_counts) * class_counts.clip(min= 1))\n",
    "sample_weights = [class_weights[x] for x in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights= torch.tensor(sample_weights), num_samples= len(sample_weights), replacement= True)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size= 16, sampler= sampler, pin_memory= True)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= 16, shuffle= False, pin_memory= True)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer= optimizer, mode= 'min', factor = 0.1, patience= 10, cooldown= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa7596",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Train Loss: 1.649261 | Val Loss: 1.540724 | Val Acc: 30.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 2/500 [01:11<4:54:35, 35.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500] Train Loss: 1.478876 | Val Loss: 1.589297 | Val Acc: 49.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 3/500 [01:46<4:51:28, 35.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500] Train Loss: 1.348680 | Val Loss: 3.359626 | Val Acc: 50.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 4/500 [02:21<4:50:48, 35.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500] Train Loss: 1.336789 | Val Loss: 1.594668 | Val Acc: 43.59%\n",
      "Epoch [5/500] Train Loss: 1.278811 | Val Loss: 1.482488 | Val Acc: 64.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 5/500 [02:56<4:50:34, 35.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/500] Train Loss: 1.232880 | Val Loss: 1.167382 | Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|▏         | 7/500 [04:06<4:48:14, 35.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/500] Train Loss: 1.076600 | Val Loss: 2.375826 | Val Acc: 52.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 8/500 [04:41<4:46:52, 34.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/500] Train Loss: 1.043973 | Val Loss: 1.185386 | Val Acc: 67.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 9/500 [05:15<4:44:36, 34.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/500] Train Loss: 1.005608 | Val Loss: 1.462796 | Val Acc: 50.00%\n",
      "Epoch [10/500] Train Loss: 1.043314 | Val Loss: 1.125653 | Val Acc: 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 11/500 [06:22<4:38:48, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/500] Train Loss: 1.111352 | Val Loss: 1.624269 | Val Acc: 56.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 12/500 [06:56<4:37:02, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/500] Train Loss: 1.006167 | Val Loss: 1.138562 | Val Acc: 65.38%\n",
      "Epoch [13/500] Train Loss: 1.016001 | Val Loss: 1.091720 | Val Acc: 73.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 13/500 [07:30<4:35:46, 33.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/500] Train Loss: 0.936975 | Val Loss: 1.039198 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 15/500 [08:37<4:32:52, 33.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/500] Train Loss: 0.899358 | Val Loss: 1.049875 | Val Acc: 78.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 16/500 [09:11<4:32:13, 33.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/500] Train Loss: 0.935630 | Val Loss: 1.079635 | Val Acc: 69.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 17/500 [09:44<4:30:07, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/500] Train Loss: 0.954444 | Val Loss: 1.179986 | Val Acc: 62.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▎         | 18/500 [10:18<4:29:46, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/500] Train Loss: 0.900169 | Val Loss: 1.548650 | Val Acc: 58.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 19/500 [10:51<4:29:13, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/500] Train Loss: 0.952169 | Val Loss: 3.620813 | Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 20/500 [11:25<4:28:17, 33.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500] Train Loss: 0.896660 | Val Loss: 1.144645 | Val Acc: 74.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 21/500 [11:58<4:27:45, 33.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/500] Train Loss: 0.854285 | Val Loss: 1.057899 | Val Acc: 73.72%\n",
      "Epoch [22/500] Train Loss: 0.868884 | Val Loss: 1.019099 | Val Acc: 71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 22/500 [12:32<4:27:44, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/500] Train Loss: 0.824946 | Val Loss: 1.001656 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▍         | 24/500 [13:39<4:26:13, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/500] Train Loss: 0.809234 | Val Loss: 1.562675 | Val Acc: 63.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 25/500 [14:12<4:25:09, 33.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/500] Train Loss: 0.845743 | Val Loss: 1.151263 | Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 26/500 [14:46<4:24:27, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/500] Train Loss: 0.818014 | Val Loss: 1.075154 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 27/500 [15:19<4:23:51, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/500] Train Loss: 0.842924 | Val Loss: 1.208645 | Val Acc: 71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 28/500 [15:53<4:23:15, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/500] Train Loss: 0.840457 | Val Loss: 1.273202 | Val Acc: 67.31%\n",
      "Epoch [29/500] Train Loss: 0.786504 | Val Loss: 0.999217 | Val Acc: 70.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 29/500 [16:27<4:23:35, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/500] Train Loss: 0.751006 | Val Loss: 0.944211 | Val Acc: 78.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 31/500 [17:34<4:22:22, 33.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/500] Train Loss: 0.777555 | Val Loss: 1.384457 | Val Acc: 56.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▋         | 32/500 [18:07<4:21:09, 33.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/500] Train Loss: 0.720574 | Val Loss: 1.319775 | Val Acc: 72.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 33/500 [18:40<4:20:33, 33.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/500] Train Loss: 0.730199 | Val Loss: 0.963314 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 34/500 [19:14<4:19:43, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/500] Train Loss: 0.754781 | Val Loss: 1.164888 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 35/500 [19:47<4:19:16, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/500] Train Loss: 0.776586 | Val Loss: 1.030581 | Val Acc: 78.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 36/500 [20:21<4:19:07, 33.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/500] Train Loss: 0.724101 | Val Loss: 1.075791 | Val Acc: 67.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 37/500 [20:54<4:18:22, 33.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/500] Train Loss: 0.767840 | Val Loss: 1.176928 | Val Acc: 70.51%\n",
      "Epoch [38/500] Train Loss: 0.754192 | Val Loss: 0.920336 | Val Acc: 75.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 39/500 [22:01<4:17:32, 33.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/500] Train Loss: 0.676148 | Val Loss: 0.963385 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 40/500 [22:35<4:16:24, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/500] Train Loss: 0.692632 | Val Loss: 0.964287 | Val Acc: 78.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 41/500 [23:08<4:15:30, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/500] Train Loss: 0.682660 | Val Loss: 1.015257 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 42/500 [23:41<4:15:01, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/500] Train Loss: 0.665139 | Val Loss: 1.053808 | Val Acc: 67.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▊         | 43/500 [24:15<4:14:18, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/500] Train Loss: 0.625454 | Val Loss: 1.182174 | Val Acc: 74.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 44/500 [24:48<4:13:54, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/500] Train Loss: 0.686373 | Val Loss: 1.292977 | Val Acc: 67.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 45/500 [25:22<4:13:34, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/500] Train Loss: 0.642957 | Val Loss: 1.182071 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 46/500 [25:55<4:12:32, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/500] Train Loss: 0.613309 | Val Loss: 1.127655 | Val Acc: 69.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 47/500 [26:28<4:11:56, 33.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/500] Train Loss: 0.653638 | Val Loss: 0.982970 | Val Acc: 81.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|▉         | 48/500 [27:02<4:11:43, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/500] Train Loss: 0.616873 | Val Loss: 1.012252 | Val Acc: 80.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|▉         | 49/500 [27:35<4:11:06, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/500] Train Loss: 0.606019 | Val Loss: 1.187934 | Val Acc: 74.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 50/500 [28:09<4:10:21, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] Train Loss: 0.592003 | Val Loss: 1.006489 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 51/500 [28:42<4:09:46, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/500] Train Loss: 0.553392 | Val Loss: 0.946103 | Val Acc: 78.21%\n",
      "Epoch [52/500] Train Loss: 0.505081 | Val Loss: 0.907238 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 53/500 [29:49<4:09:13, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/500] Train Loss: 0.526239 | Val Loss: 0.980119 | Val Acc: 78.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 54/500 [30:22<4:08:23, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/500] Train Loss: 0.510860 | Val Loss: 0.947907 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 55/500 [30:56<4:07:54, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/500] Train Loss: 0.501557 | Val Loss: 0.974718 | Val Acc: 80.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 56/500 [31:29<4:07:20, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/500] Train Loss: 0.490652 | Val Loss: 0.929880 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█▏        | 57/500 [32:02<4:06:25, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/500] Train Loss: 0.485907 | Val Loss: 0.950449 | Val Acc: 81.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 58/500 [32:36<4:06:03, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/500] Train Loss: 0.465906 | Val Loss: 0.935986 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 59/500 [33:09<4:05:33, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/500] Train Loss: 0.480837 | Val Loss: 0.948123 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 60/500 [33:43<4:04:35, 33.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/500] Train Loss: 0.472331 | Val Loss: 0.996262 | Val Acc: 79.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 61/500 [34:16<4:04:52, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/500] Train Loss: 0.448774 | Val Loss: 0.944911 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 62/500 [34:50<4:04:32, 33.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/500] Train Loss: 0.447713 | Val Loss: 1.000380 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 63/500 [35:23<4:03:17, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/500] Train Loss: 0.470841 | Val Loss: 0.960368 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 64/500 [35:57<4:02:54, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/500] Train Loss: 0.462609 | Val Loss: 1.015559 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 65/500 [36:30<4:02:37, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/500] Train Loss: 0.465878 | Val Loss: 0.977501 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 66/500 [37:03<4:01:36, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/500] Train Loss: 0.444459 | Val Loss: 0.972981 | Val Acc: 79.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 67/500 [37:37<4:00:42, 33.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/500] Train Loss: 0.459915 | Val Loss: 0.997578 | Val Acc: 80.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▎        | 68/500 [38:10<4:00:09, 33.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/500] Train Loss: 0.446898 | Val Loss: 0.939826 | Val Acc: 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 69/500 [38:43<3:59:22, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/500] Train Loss: 0.435767 | Val Loss: 0.951759 | Val Acc: 80.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 70/500 [39:17<3:58:48, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/500] Train Loss: 0.449346 | Val Loss: 0.970975 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 71/500 [39:50<3:58:46, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/500] Train Loss: 0.459629 | Val Loss: 0.985116 | Val Acc: 81.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 71/500 [40:23<4:04:05, 34.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/500] Train Loss: 0.431892 | Val Loss: 0.996667 | Val Acc: 78.85%\n",
      "Early stopping at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks, labels in train_loader:\n",
    "        images = images.to('cuda')\n",
    "        masks = masks.to('cuda').view(-1, 3, 256, 256).float()\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mask_pred, output_class_logits = model(images)\n",
    "        loss = criterion(mask_pred, masks, output_class_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels in test_loader:\n",
    "            images = images.to('cuda')\n",
    "            masks = masks.to('cuda').view(-1, 3, 256, 256).float()\n",
    "            labels = labels.to('cuda')\n",
    "            mask_pred, output_class_logits = model(images)\n",
    "            loss = criterion(mask_pred, masks, output_class_logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            output_classes = output_class_logits.argmax(dim= 1)\n",
    "            correct += (output_classes == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.6f} | Val Loss: {val_loss:.6f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    train_loss.append(avg_loss)\n",
    "    test_loss.append(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6260c80",
   "metadata": {},
   "source": [
    "### Saving loss lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_pd = pd.DataFrame(train_loss)\n",
    "train_pd.to_csv('train_loss.csv', index= False, header= False)\n",
    "test_pd = pd.DataFrame(test_loss)\n",
    "test_pd.to_csv('test_loss.csv', index= False, header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassPrecision\n",
    "\n",
    "model.eval()\n",
    "preds, targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks, labels in test_loader:\n",
    "        images = images.to('cuda')\n",
    "        masks = masks.to('cuda').view(-1, 3, 256, 256).float()\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        seg_logits, cls_logits = model(images)\n",
    "        pred_cls = cls_logits.argmax(dim=1)  # (B,)\n",
    "\n",
    "        preds.append(pred_cls.cpu())\n",
    "        targets.append(labels.cpu())\n",
    "\n",
    "preds = torch.cat(preds)      # shape (N,)\n",
    "targets = torch.cat(targets)  # shape (N,)\n",
    "\n",
    "metric = MulticlassPrecision(num_classes=3, average=\"macro\")\n",
    "precision_macro = metric(preds, targets).item()\n",
    "print(\"Macro Precision:\", precision_macro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312b843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
