{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ce3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import defaultdict\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a5701",
   "metadata": {},
   "source": [
    "### Finding different shapes and channels of images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05d8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape length - 639 and mask shape length - 639\n",
      "number of channels - 3\n",
      "channels - {'1', 'RGB', 'RGBA'}\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'D:\\Suchit\\Breast-Cancer-Detection\\Dataset_BUSI_with_GT'\n",
    "image_shapes, mask_shapes = set(), set()\n",
    "channels = set()\n",
    "images, masks = [], []\n",
    "for folder in os.listdir(folder_path):\n",
    "    for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "        with Image.open(os.path.join(folder_path, folder, image)) as img:\n",
    "            if 'mask' in image:\n",
    "                mask_shapes.add(img.size)\n",
    "                masks.append(os.path.join(folder_path, folder, image))\n",
    "            else:\n",
    "                image_shapes.add(img.size)\n",
    "                images.append(os.path.join(folder_path, folder, image))\n",
    "            channels.add(img.mode)\n",
    "print(\n",
    "    f'image shape length - {len(image_shapes)} and mask shape length - {len(mask_shapes)}')\n",
    "print(f'number of channels - {len(channels)}')\n",
    "print(f'channels - {channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36eff8b",
   "metadata": {},
   "source": [
    "### Finding the average height and width of the images to resize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64e5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average height = 501.4525641025641\n",
      "average width = 615.6794871794872\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'D:\\Suchit\\Breast-Cancer-Detection\\Dataset_BUSI_with_GT'\n",
    "height, width, num_samples = 0.0, 0.0, 0.0\n",
    "for folder in os.listdir(folder_path):\n",
    "    for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "        with Image.open(os.path.join(folder_path, folder, image)) as img:\n",
    "            if 'mask' not in image:\n",
    "                size = img.size\n",
    "                height += size[1]\n",
    "                width += size[0]\n",
    "                num_samples += 1\n",
    "height /= num_samples\n",
    "width /= num_samples\n",
    "print(f'average height = {height}')\n",
    "print(f'average width = {width}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2f6cd",
   "metadata": {},
   "source": [
    "Height and width is too large. Taking 256 * 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f3200",
   "metadata": {},
   "source": [
    "### Finding the mean and standard deviation of the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8071d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating grayscale statistics...\n",
      "Found 780 images for statistics calculation\n",
      "Grayscale mean = 0.3279, std = 0.1998\n"
     ]
    }
   ],
   "source": [
    "def calculate_grayscale_stats(folder_path, resize_shape=(256, 256)):\n",
    "    \"\"\"Calculate mean and std for grayscale images\"\"\"\n",
    "    print(\"Calculating grayscale statistics...\")\n",
    "\n",
    "    # Simple transform for grayscale statistics calculation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(resize_shape),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    class GrayscaleImageData(Dataset):\n",
    "        def __init__(self, images, transform):\n",
    "            self.images = images\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img_path = self.images[index]\n",
    "            image = Image.open(img_path)\n",
    "            return self.transform(image)\n",
    "\n",
    "    # Get image paths (exclude masks)\n",
    "    images = []\n",
    "    for folder in os.listdir(folder_path):\n",
    "        for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "            if 'mask' not in image:\n",
    "                images.append(os.path.join(folder_path, folder, image))\n",
    "\n",
    "    print(f\"Found {len(images)} images for statistics calculation\")\n",
    "\n",
    "    dataset = GrayscaleImageData(images, transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    mean, std, num_samples = 0.0, 0.0, 0.0\n",
    "\n",
    "    for data in loader:\n",
    "        batch_size = data.size(0)\n",
    "        data = data.view(batch_size, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        num_samples += batch_size\n",
    "\n",
    "    mean /= num_samples\n",
    "    std /= num_samples\n",
    "\n",
    "    return mean.item(), std.item()\n",
    "\n",
    "\n",
    "# Step 2: Calculate grayscale statistics for your dataset\n",
    "grayscale_mean, grayscale_std = calculate_grayscale_stats(folder_path)\n",
    "print(f'Grayscale mean = {grayscale_mean:.4f}, std = {grayscale_std:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696dbce",
   "metadata": {},
   "source": [
    "### Managing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97e944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, folder_path, transforms):\n",
    "        super().__init__()\n",
    "        self.images, self.masks, self.category = [], defaultdict(list), []\n",
    "        classes = {'benign': 0, 'malignant': 1, 'normal': 2}\n",
    "        self.transforms = transforms\n",
    "\n",
    "        for folder in os.listdir(folder_path):\n",
    "            for image in os.listdir(os.path.join(folder_path, folder)):\n",
    "                img_path = os.path.join(folder_path, folder, image)\n",
    "                if 'mask' in image:\n",
    "                    self.masks[image[: image.index('_mask')]].append(img_path)\n",
    "                else:\n",
    "                    self.images.append(img_path)\n",
    "                    self.category.append(classes[folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        key = os.path.basename(image_path)[\n",
    "            : os.path.basename(image_path).index('.png')]\n",
    "        mask_paths = self.masks[key]\n",
    "\n",
    "        # Load image and masks as numpy arrays\n",
    "        image = np.array(Image.open(image_path).convert('L'))\n",
    "        masks = [np.array(Image.open(x).convert('L')) for x in mask_paths]\n",
    "\n",
    "        # Combine masks\n",
    "        mask = np.sum(np.stack(masks), axis=0)\n",
    "        mask = np.clip(mask, 0, 1)  # ensure binary\n",
    "\n",
    "        # Apply albumentations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask, self.category[index]\n",
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=[grayscale_mean], std=[grayscale_std]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "dataset = Data(folder_path, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ed3d7",
   "metadata": {},
   "source": [
    "### Calculating positive weight to upweight the positive pixels in BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41b07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated positive weight = 11.776581764221191\n"
     ]
    }
   ],
   "source": [
    "def estimate_pos_weight(dataset):\n",
    "    loader = DataLoader(dataset= dataset, batch_size=16, shuffle= False)\n",
    "    pos, neg = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for _, mask, _ in loader:\n",
    "            mask = mask.flatten()\n",
    "            mask = (mask > 0)\n",
    "            pos += mask.sum().item()\n",
    "            neg += mask.numel() - mask.sum().item()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0)\n",
    "    return torch.tensor(neg / pos)\n",
    "\n",
    "# making counting transforms for counting the pos weights\n",
    "\n",
    "count_transform = A.Compose([\n",
    "    A.Resize(256, 256, interpolation= cv2.INTER_NEAREST, mask_interpolation= cv2.INTER_NEAREST),\n",
    "    A.Normalize(mean = [grayscale_mean], std= [grayscale_std]),\n",
    "    A.pytorch.ToTensorV2()\n",
    "])\n",
    "count_dataset = Data(folder_path= folder_path, transforms= count_transform)\n",
    "pos_weight = estimate_pos_weight(count_dataset)\n",
    "pos_weight = pos_weight.to('cuda')\n",
    "print(f'Estimated positive weight = {pos_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fae8b0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d1a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_unet_kaiming(module):\n",
    "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        if module.weight is not None:\n",
    "            nn.init.kaiming_normal_(module.weight, nonlinearity= 'relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        if module.weight is not None:\n",
    "            nn.init.kaiming_normal_(module.weight, nonlinearity= 'relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        if getattr(module, 'weight', None) is not None:\n",
    "            nn.init.ones_(module.weight)\n",
    "        if getattr(module, 'bias', None) is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "def zero_init_seg_head(head):\n",
    "    if isinstance(head, nn.Conv2d):\n",
    "        nn.init.zeros_(head.weight)\n",
    "        if head.bias is not None:\n",
    "            nn.init.zeros_(head.bias)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels,\n",
    "                      kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetWithClassifier(nn.Module):\n",
    "    def __init__(self, in_channels=1, seg_out_channels=1, num_classes=3, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Decoder (with concatenations)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        self.dec4 = DoubleConv(1024, 512)   # 512 up + 512 skip\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.dec3 = DoubleConv(512, 256)    # 256 up + 256 skip\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.dec2 = DoubleConv(256, 128)    # 128 up + 128 skip\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.dec1 = DoubleConv(128, 64)     # 64 up + 64 skip\n",
    "\n",
    "        self.seg_head = nn.Conv2d(64, seg_out_channels, kernel_size=1)\n",
    "\n",
    "        # Classification head on bottleneck features\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512, bias=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.apply(init_unet_kaiming)\n",
    "        zero_init_seg_head(self.seg_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        p1 = self.pool1(d1)\n",
    "\n",
    "        d2 = self.down2(p1)\n",
    "        p2 = self.pool2(d2)\n",
    "\n",
    "        d3 = self.down3(p2)\n",
    "        p3 = self.pool3(d3)\n",
    "\n",
    "        d4 = self.down4(p3)\n",
    "        p4 = self.pool4(d4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        u4 = self.up4(bn)\n",
    "        u4 = torch.cat([u4, d4], dim=1)\n",
    "        u4 = self.dec4(u4)\n",
    "\n",
    "        u3 = self.up3(u4)\n",
    "        u3 = torch.cat([u3, d3], dim=1)\n",
    "        u3 = self.dec3(u3)\n",
    "\n",
    "        u2 = self.up2(u3)\n",
    "        u2 = torch.cat([u2, d2], dim=1)\n",
    "        u2 = self.dec2(u2)\n",
    "\n",
    "        u1 = self.up1(u2)\n",
    "        u1 = torch.cat([u1, d1], dim=1)\n",
    "        u1 = self.dec1(u1)\n",
    "\n",
    "        seg_logits = self.seg_head(u1)\n",
    "        cls_logits = self.cls_head(self.gap(bn))\n",
    "        return seg_logits, cls_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd28d2",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ea128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth = 1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, output_mask, targets):\n",
    "        targets = targets.float()\n",
    "        probs = torch.sigmoid(output_mask)\n",
    "        # no. of batches\n",
    "        B = output_mask.shape[0]\n",
    "        probs_flatten = probs.view(B, -1)\n",
    "        targs_flatten = targets.view(B, -1)\n",
    "        intersection = (probs_flatten * targs_flatten).sum(dim = 1)\n",
    "        p_sum = probs_flatten.sum(dim = 1)\n",
    "        t_sum = targs_flatten.sum(dim = 1)\n",
    "        # identifying empty target images\n",
    "        empty = (t_sum == 0)\n",
    "        # standard dice for non empty\n",
    "        dice_non_empty = (2 * intersection + self.smooth) / (p_sum + t_sum + self.smooth)\n",
    "        # defining dice for empty target images\n",
    "        # if predictions also empty -> 1, else -> (smooth) / (psum + smooth)\n",
    "        dice_empty = torch.where((p_sum == 0), torch.ones_like(p_sum), (self.smooth) / (p_sum + self.smooth))\n",
    "        dice = torch.where(empty, dice_empty, dice_non_empty)\n",
    "        return 1 - dice.mean()\n",
    "    \n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, bce_pos_weight):\n",
    "        super().__init__()\n",
    "        self.seg_weight = 1.0\n",
    "        self.cls_weight = 0.5\n",
    "        self.bce_weight = 0.5\n",
    "        self.dice_weight = 0.5\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight= bce_pos_weight)\n",
    "        self.dice = DiceLoss()\n",
    "        self.crossentropy = nn.CrossEntropyLoss()\n",
    "    def forward(self, output_mask, target_mask, output_class_logits, class_logits):\n",
    "        loss_bce = self.bce(output_mask, target_mask)\n",
    "        loss_dice = self.dice(output_mask, target_mask)\n",
    "        \n",
    "        loss_mask = self.bce_weight * loss_bce + self.dice_weight * loss_dice\n",
    "\n",
    "        loss_classification = self.crossentropy(output_class_logits  , class_logits.long())\n",
    "\n",
    "        total = self.seg_weight * loss_mask + self.cls_weight * loss_classification\n",
    "\n",
    "        return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaddd36",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae00a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = UNetWithClassifier()\n",
    "model = model.to('cuda')\n",
    "\n",
    "# loss function\n",
    "criterion = MultiTaskLoss(pos_weight)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 0.001, weight_decay= 0.0001)\n",
    "\n",
    "# train test split\n",
    "n_total = len(dataset)\n",
    "n_val = int(0.2 * n_total)\n",
    "n_train = n_total - n_val\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val], generator= torch.Generator().manual_seed(42))\n",
    "\n",
    "# weighted random sampler\n",
    "labels = np.array([dataset.category[i] for i in train_dataset.indices])\n",
    "class_counts = np.bincount(labels, minlength= 3)\n",
    "class_weights = class_counts.sum() / (len(class_counts) * class_counts.clip(min= 1))\n",
    "sample_weights = [class_weights[x] for x in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights= torch.tensor(sample_weights), num_samples= len(sample_weights), replacement= True)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size= 16, sampler= sampler, pin_memory= True)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= 16, shuffle= False, pin_memory= True)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer= optimizer, mode= 'min', factor = 0.1, patience= 10, cooldown= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa7596",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d53bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Train Loss: 1.700441 | Val Loss: 1.677556 | Val Acc: 47.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 1/500 [00:36<5:00:13, 36.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500] Train Loss: 1.464693 | Val Loss: 1.584409 | Val Acc: 54.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 2/500 [01:11<4:56:08, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500] Train Loss: 1.400320 | Val Loss: 1.578504 | Val Acc: 37.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 3/500 [01:46<4:52:25, 35.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500] Train Loss: 1.359732 | Val Loss: 1.498771 | Val Acc: 51.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 4/500 [02:21<4:50:02, 35.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/500] Train Loss: 1.234397 | Val Loss: 1.299418 | Val Acc: 68.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 5/500 [02:55<4:47:41, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/500] Train Loss: 1.276256 | Val Loss: 1.198629 | Val Acc: 73.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|▏         | 7/500 [04:04<4:44:36, 34.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/500] Train Loss: 1.211137 | Val Loss: 1.947131 | Val Acc: 58.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 8/500 [04:38<4:43:10, 34.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/500] Train Loss: 1.174480 | Val Loss: 1.267398 | Val Acc: 73.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 9/500 [05:13<4:41:54, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/500] Train Loss: 1.195057 | Val Loss: 1.819574 | Val Acc: 44.23%\n",
      "Epoch [10/500] Train Loss: 1.121153 | Val Loss: 1.175433 | Val Acc: 71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 10/500 [05:48<4:42:26, 34.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/500] Train Loss: 1.160682 | Val Loss: 1.142873 | Val Acc: 76.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 12/500 [06:56<4:40:27, 34.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/500] Train Loss: 1.072226 | Val Loss: 1.681926 | Val Acc: 56.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 13/500 [07:31<4:39:20, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/500] Train Loss: 1.085912 | Val Loss: 1.145668 | Val Acc: 69.23%\n",
      "Epoch [14/500] Train Loss: 1.082605 | Val Loss: 1.092855 | Val Acc: 73.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 15/500 [08:40<4:38:23, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/500] Train Loss: 1.018676 | Val Loss: 1.158118 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 16/500 [09:14<4:37:28, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/500] Train Loss: 1.018000 | Val Loss: 1.151738 | Val Acc: 71.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 17/500 [09:48<4:35:52, 34.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/500] Train Loss: 1.121794 | Val Loss: 1.883450 | Val Acc: 51.28%\n",
      "Epoch [18/500] Train Loss: 1.010612 | Val Loss: 1.029896 | Val Acc: 79.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 19/500 [10:56<4:34:26, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/500] Train Loss: 0.973401 | Val Loss: 1.113977 | Val Acc: 76.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 20/500 [11:31<4:34:02, 34.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500] Train Loss: 1.016933 | Val Loss: 1.116111 | Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 21/500 [12:05<4:33:20, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/500] Train Loss: 0.979032 | Val Loss: 1.430634 | Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 22/500 [12:39<4:32:49, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/500] Train Loss: 0.918728 | Val Loss: 1.151081 | Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▍         | 23/500 [13:14<4:32:45, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/500] Train Loss: 0.998915 | Val Loss: 1.129789 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▍         | 24/500 [13:48<4:32:05, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/500] Train Loss: 0.926486 | Val Loss: 1.207250 | Val Acc: 67.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 25/500 [14:22<4:31:46, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/500] Train Loss: 0.895685 | Val Loss: 1.494757 | Val Acc: 55.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 26/500 [14:56<4:30:51, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/500] Train Loss: 0.940578 | Val Loss: 1.604716 | Val Acc: 62.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 27/500 [15:31<4:30:37, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/500] Train Loss: 0.948557 | Val Loss: 1.103533 | Val Acc: 73.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 28/500 [16:05<4:29:18, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/500] Train Loss: 0.898022 | Val Loss: 1.227427 | Val Acc: 64.74%\n",
      "Epoch [29/500] Train Loss: 0.846633 | Val Loss: 0.996716 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▌         | 30/500 [17:13<4:28:19, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/500] Train Loss: 0.900557 | Val Loss: 1.079801 | Val Acc: 76.92%\n",
      "Epoch [31/500] Train Loss: 0.843697 | Val Loss: 0.987540 | Val Acc: 81.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|▋         | 32/500 [18:22<4:27:40, 34.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/500] Train Loss: 0.873842 | Val Loss: 1.567691 | Val Acc: 70.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 33/500 [18:57<4:27:55, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/500] Train Loss: 0.828916 | Val Loss: 1.081380 | Val Acc: 75.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 34/500 [19:31<4:26:59, 34.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/500] Train Loss: 0.838750 | Val Loss: 1.226299 | Val Acc: 62.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 35/500 [20:06<4:26:37, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/500] Train Loss: 0.865840 | Val Loss: 1.075177 | Val Acc: 74.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 36/500 [20:40<4:26:24, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/500] Train Loss: 0.827207 | Val Loss: 1.018391 | Val Acc: 78.21%\n",
      "Epoch [37/500] Train Loss: 0.793512 | Val Loss: 0.968278 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 38/500 [21:49<4:25:32, 34.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/500] Train Loss: 0.743244 | Val Loss: 1.091124 | Val Acc: 77.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 39/500 [22:24<4:24:35, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/500] Train Loss: 0.733150 | Val Loss: 1.090485 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 40/500 [22:58<4:23:11, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/500] Train Loss: 0.780034 | Val Loss: 1.024496 | Val Acc: 78.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 41/500 [23:32<4:22:29, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/500] Train Loss: 0.775828 | Val Loss: 1.114881 | Val Acc: 76.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 42/500 [24:06<4:22:10, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/500] Train Loss: 0.746993 | Val Loss: 1.394185 | Val Acc: 53.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▊         | 43/500 [24:41<4:21:43, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/500] Train Loss: 0.737345 | Val Loss: 1.510013 | Val Acc: 65.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 44/500 [25:15<4:20:44, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/500] Train Loss: 0.790830 | Val Loss: 1.124319 | Val Acc: 73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 45/500 [25:49<4:20:25, 34.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/500] Train Loss: 0.710354 | Val Loss: 1.077346 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 46/500 [26:24<4:19:27, 34.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/500] Train Loss: 0.689717 | Val Loss: 1.076236 | Val Acc: 81.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 47/500 [26:58<4:19:09, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/500] Train Loss: 0.729000 | Val Loss: 1.057510 | Val Acc: 77.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|▉         | 48/500 [27:32<4:17:56, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/500] Train Loss: 0.737779 | Val Loss: 1.012191 | Val Acc: 74.36%\n",
      "Epoch [49/500] Train Loss: 0.693318 | Val Loss: 0.924773 | Val Acc: 79.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|▉         | 49/500 [28:07<4:18:10, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] Train Loss: 0.663049 | Val Loss: 0.914933 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 50/500 [28:41<4:18:20, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/500] Train Loss: 0.635165 | Val Loss: 0.913651 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 52/500 [29:50<4:16:32, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/500] Train Loss: 0.636922 | Val Loss: 0.914712 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 53/500 [30:24<4:16:07, 34.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/500] Train Loss: 0.620401 | Val Loss: 0.943523 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 54/500 [30:59<4:15:38, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/500] Train Loss: 0.610525 | Val Loss: 0.924923 | Val Acc: 82.69%\n",
      "Epoch [55/500] Train Loss: 0.607270 | Val Loss: 0.903259 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█         | 56/500 [32:07<4:14:31, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/500] Train Loss: 0.603466 | Val Loss: 0.946217 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  11%|█▏        | 57/500 [32:42<4:13:21, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/500] Train Loss: 0.617593 | Val Loss: 0.927084 | Val Acc: 82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 58/500 [33:16<4:12:29, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/500] Train Loss: 0.604987 | Val Loss: 0.910957 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 59/500 [33:50<4:11:56, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/500] Train Loss: 0.586503 | Val Loss: 0.938828 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 60/500 [34:24<4:10:58, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/500] Train Loss: 0.588000 | Val Loss: 0.952171 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 61/500 [34:58<4:10:23, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/500] Train Loss: 0.573666 | Val Loss: 0.940851 | Val Acc: 85.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 62/500 [35:33<4:09:59, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/500] Train Loss: 0.580069 | Val Loss: 0.954661 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 63/500 [36:07<4:09:11, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/500] Train Loss: 0.565297 | Val Loss: 0.934039 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 64/500 [36:41<4:08:34, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/500] Train Loss: 0.571838 | Val Loss: 0.972566 | Val Acc: 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 65/500 [37:15<4:08:01, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/500] Train Loss: 0.569971 | Val Loss: 1.007251 | Val Acc: 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 66/500 [37:49<4:07:19, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/500] Train Loss: 0.549311 | Val Loss: 0.981265 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 67/500 [38:24<4:06:50, 34.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/500] Train Loss: 0.559916 | Val Loss: 0.973305 | Val Acc: 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▎        | 68/500 [38:58<4:06:13, 34.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/500] Train Loss: 0.545910 | Val Loss: 0.980693 | Val Acc: 85.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 69/500 [39:32<4:05:56, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/500] Train Loss: 0.569734 | Val Loss: 0.970382 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 70/500 [40:07<4:06:01, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/500] Train Loss: 0.593091 | Val Loss: 0.966506 | Val Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 71/500 [40:41<4:06:05, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/500] Train Loss: 0.536049 | Val Loss: 0.977694 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|█▍        | 72/500 [41:16<4:05:55, 34.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/500] Train Loss: 0.544787 | Val Loss: 0.965431 | Val Acc: 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▍        | 73/500 [41:51<4:05:39, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/500] Train Loss: 0.563124 | Val Loss: 0.986946 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▍        | 74/500 [42:25<4:05:16, 34.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/500] Train Loss: 0.576185 | Val Loss: 0.972137 | Val Acc: 83.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▍        | 74/500 [43:00<4:07:32, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/500] Train Loss: 0.567214 | Val Loss: 0.986857 | Val Acc: 85.26%\n",
      "Early stopping at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "train_loss, test_loss = [], []\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks, labels in train_loader:\n",
    "        images = images.to('cuda')\n",
    "        masks = masks.to('cuda').view(-1, 1, 256, 256).float()\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mask_pred, output_class_logits = model(images)\n",
    "        loss = criterion(mask_pred, masks, output_class_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels in test_loader:\n",
    "            images = images.to('cuda')\n",
    "            masks = masks.to('cuda').view(-1, 1, 256, 256).float()\n",
    "            labels = labels.to('cuda')\n",
    "            mask_pred, output_class_logits = model(images)\n",
    "            loss = criterion(mask_pred, masks, output_class_logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            output_classes = output_class_logits.argmax(dim= 1)\n",
    "            correct += (output_classes == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.6f} | Val Loss: {val_loss:.6f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    train_loss.append(avg_loss)\n",
    "    test_loss.append(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6260c80",
   "metadata": {},
   "source": [
    "### Saving loss lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e83b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_pd = pd.DataFrame(train_loss)\n",
    "train_pd.to_csv('train_loss.csv', index= False, header= False)\n",
    "test_pd = pd.DataFrame(test_loss)\n",
    "test_pd.to_csv('test_loss.csv', index= False, header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d02d624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.8583203554153442\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassPrecision\n",
    "\n",
    "model.eval()\n",
    "preds, targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks, labels in test_loader:\n",
    "        images = images.to('cuda')\n",
    "        masks = masks.to('cuda').view(-1, 1, 256, 256).float()\n",
    "        labels = labels.to('cuda')\n",
    "\n",
    "        seg_logits, cls_logits = model(images)\n",
    "        pred_cls = cls_logits.argmax(dim=1)  # (B,)\n",
    "\n",
    "        preds.append(pred_cls.cpu())\n",
    "        targets.append(labels.cpu())\n",
    "\n",
    "preds = torch.cat(preds)      # shape (N,)\n",
    "targets = torch.cat(targets)  # shape (N,)\n",
    "\n",
    "metric = MulticlassPrecision(num_classes=3, average=\"macro\")\n",
    "precision_macro = metric(preds, targets).item()\n",
    "print(\"Macro Precision:\", precision_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204a75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro=0.8583  micro=0.8462  weighted=0.8479\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassPrecision\n",
    "\n",
    "prec_macro = MulticlassPrecision(num_classes=3, average=\"macro\")(preds, targets).item()\n",
    "prec_micro = MulticlassPrecision(num_classes=3, average=\"micro\")(preds, targets).item()\n",
    "prec_weighted = MulticlassPrecision(num_classes=3, average=\"weighted\")(preds, targets).item()\n",
    "print(f\"macro={prec_macro:.4f}  micro={prec_micro:.4f}  weighted={prec_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84b79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════╤═════════╕\n",
      "│ Metric            │   Score │\n",
      "╞═══════════════════╪═════════╡\n",
      "│ Accuracy          │  0.8269 │\n",
      "├───────────────────┼─────────┤\n",
      "│ Precision (macro) │  0.8335 │\n",
      "├───────────────────┼─────────┤\n",
      "│ Recall (macro)    │  0.83   │\n",
      "├───────────────────┼─────────┤\n",
      "│ F1 (macro)        │  0.8316 │\n",
      "╘═══════════════════╧═════════╛\n",
      "╒═════════╤═════════════╤══════════╤════════╕\n",
      "│   Class │   Precision │   Recall │     F1 │\n",
      "╞═════════╪═════════════╪══════════╪════════╡\n",
      "│       0 │      0.8333 │   0.8333 │ 0.8333 │\n",
      "├─────────┼─────────────┼──────────┼────────┤\n",
      "│       1 │      0.7609 │   0.7778 │ 0.7692 │\n",
      "├─────────┼─────────────┼──────────┼────────┤\n",
      "│       2 │      0.9062 │   0.8788 │ 0.8923 │\n",
      "╘═════════╧═════════════╧══════════╧════════╛\n"
     ]
    }
   ],
   "source": [
    "statedict = torch.load('model.pt')\n",
    "model = UNetWithClassifier()\n",
    "model = model.to('cuda')\n",
    "model.load_state_dict(statedict)\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    "    MulticlassF1Score,\n",
    ")\n",
    "from tabulate import tabulate\n",
    "\n",
    "def evaluate_metrics(model, dataloader, device=\"cuda\", num_classes=3):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            _, cls_logits = model(images)\n",
    "            pred_cls = cls_logits.argmax(dim=1)\n",
    "\n",
    "            preds.append(pred_cls.cpu())\n",
    "            targets.append(labels.cpu())\n",
    "\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "\n",
    "    # Macro / Micro / Weighted\n",
    "    metrics = {\n",
    "        \"Accuracy\": MulticlassAccuracy(num_classes=num_classes, average=\"micro\")(preds, targets).item(),\n",
    "        \"Precision (macro)\": MulticlassPrecision(num_classes=num_classes, average=\"macro\")(preds, targets).item(),\n",
    "        \"Recall (macro)\": MulticlassRecall(num_classes=num_classes, average=\"macro\")(preds, targets).item(),\n",
    "        \"F1 (macro)\": MulticlassF1Score(num_classes=num_classes, average=\"macro\")(preds, targets).item(),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    per_class_prec = MulticlassPrecision(num_classes=num_classes, average=None)(preds, targets)\n",
    "    per_class_rec  = MulticlassRecall(num_classes=num_classes, average=None)(preds, targets)\n",
    "    per_class_f1   = MulticlassF1Score(num_classes=num_classes, average=None)(preds, targets)\n",
    "\n",
    "    # Print summary table\n",
    "    summary_table = [[k, f\"{v:.4f}\"] for k, v in metrics.items()]\n",
    "    print(tabulate(summary_table, headers=[\"Metric\", \"Score\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    # Print per-class table\n",
    "    class_table = []\n",
    "    for i in range(num_classes):\n",
    "        class_table.append([i, f\"{per_class_prec[i]:.4f}\", f\"{per_class_rec[i]:.4f}\", f\"{per_class_f1[i]:.4f}\"])\n",
    "    print(tabulate(class_table, headers=[\"Class\", \"Precision\", \"Recall\", \"F1\"], tablefmt=\"fancy_grid\"))\n",
    "evaluate_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7b416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
